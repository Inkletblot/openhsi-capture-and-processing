{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0a1601-eb7a-46b9-8206-ef03e65db324",
   "metadata": {},
   "source": [
    "# SARDI Capture Evaluation\n",
    "\n",
    "First a qualitative comparison of rgb image, averaged 'ground truth', the original image, and the classified result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d964e1-ada1-4b80-8f86-04e8b8d41f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import hyde\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from spectral import *\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from skimage.segmentation import slic, mark_boundaries\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48b1ea9-5201-4b19-8b4b-fad9f14d30a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iteration='1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d239b5b-54f2-4b68-8e0a-123642b9654c",
   "metadata": {},
   "source": [
    "List out all files in directory to process for easier use, recall, every second capture is aligned the same starting at capture 2 (1 indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25805868-5ac1-479c-8a9e-fa7dc81b58bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"processed-data/sardi-capture/capture-3/reflectance/2023_02_28/\"\n",
    "\n",
    "dir_cont = os.listdir(path)\n",
    "\n",
    "data_cubes = [cont for cont in dir_cont if '.nc' in cont]\n",
    "\n",
    "data_cubes = sorted(data_cubes)\n",
    "\n",
    "cubes = []\n",
    "\n",
    "for i in range(len(data_cubes)):    \n",
    "    # capture every second item\n",
    "    if i%2 == 1:\n",
    "        cubes.append(data_cubes[i])\n",
    "\n",
    "print(cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f063d0a-3ecf-49c6-a817-ec1fcb2acf7d",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Here I'm going to use my pre-processed files containing reflectance values.\n",
    "\n",
    "Given the fact that OpenHSI saves them in NetCDF format I'm using the `netCDF4` library to load them.\n",
    "\n",
    "I'm also transposing the data array into a more comment layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0649a-0604-4c30-87a4-18a8bff164da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = path + \"2023_02_28-01_03_28.nc\"\n",
    "fn_rad = fn.replace('reflectance','radiance')\n",
    "\n",
    "gnd_t_fn = \"right-pipe-gt.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f52ed-6b3d-44ee-a35a-d238867007a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "ds = nc.Dataset(fn)\n",
    "ds_rad = nc.Dataset(fn_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d4f0d-84b6-4fa0-8ca4-661811953ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the datacube from the dataset\n",
    "dc = np.array(ds['datacube'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52735961-0a6a-4a04-ae28-9f296a4fe5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data needs to be x, y, bands instead of bands, x, y.\n",
    "data = dc.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73258f-6eea-4879-a28c-4e0e584a25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit data to area of interest, so that gt lines up\n",
    "# these values are taken from the gt-average notebook processing\n",
    "# the x value cuts off the noise from the camera at the bottom of the image\n",
    "# the y values center the pipe among all of the images\n",
    "data = data[:430,:930, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f5c1a-7421-4c16-b5e4-4de0e9a1378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into a line of pixels for PCA\n",
    "X = data.copy().reshape(data.shape[0]*data.shape[1],data.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d302725-ec7e-4820-8d89-84f2e8c468fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37453ef3-b416-4612-9c92-1e2bfb444331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "with open(gnd_t_fn, 'rb') as out_file:\n",
    "    ground_truth = pickle.load(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924467f7-5f7d-40a0-8abe-5e8407eb5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ground_truth.copy().reshape(ground_truth.shape[0]*ground_truth.shape[1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f4003-d7fd-487b-9cd6-30aed84ea267",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481185de-858f-425a-9044-a9d1d6f929a9",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6574b57-5b53-4e67-823f-0a96c32ab919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_into_rgb(loc_ds):\n",
    "    \"\"\"\n",
    "    Processes a given dataset into an rgb image for display\n",
    "    \"\"\"\n",
    "    loc_dc = np.array(loc_ds['datacube'])\n",
    "    data = loc_dc.transpose(1,2,0)\n",
    "    data = data[:430,:930, :] # limit to area we are interested in\n",
    "    loc_binned_wavelengths = np.array(loc_ds['wavelength'])\n",
    "    loc_image = np.zeros((data.shape[0],data.shape[1],3), np.uint8)\n",
    "    \n",
    "    # This is not working, it could be a ram issue\n",
    "    # input_tens = torch.tensor(data, dtype=torch.float32, device=\"cpu\")\n",
    "    # hyres = hyde.HyRes()\n",
    "    # dn_data = hyres(input_tens).numpy()\n",
    "    dn_data = data\n",
    "\n",
    "    loc_red_wavelength = 620\n",
    "    loc_green_wavelength = 560\n",
    "    loc_blue_wavelength = 460\n",
    "\n",
    "    loc_red_layer = dn_data[:,:,np.argmin(np.abs(loc_binned_wavelengths-loc_red_wavelength))]  \n",
    "    loc_green_layer = dn_data[:,:,np.argmin(np.abs(loc_binned_wavelengths-loc_green_wavelength))] \n",
    "    loc_blue_layer = dn_data[:,:,np.argmin(np.abs(loc_binned_wavelengths-loc_blue_wavelength))]\n",
    "    \n",
    "    loc_image[:,:,0] = loc_red_layer\n",
    "    loc_image[:,:,1] = loc_green_layer\n",
    "    loc_image[:,:,2] = loc_blue_layer\n",
    "\n",
    "    # scale the values to within the central 2 to 98 percent values to remove outlier readings\n",
    "    loc_vmax = np.nanpercentile(loc_image, 98)\n",
    "    loc_vmin = np.nanpercentile(loc_image, 2)\n",
    "    loc_image = ((loc_image.astype(\"f8\") - loc_vmin) / (loc_vmax - loc_vmin)).astype(\"f4\")\n",
    "    loc_image = np.minimum(np.maximum(loc_image, 0), 1)\n",
    "\n",
    "    loc_image *= 255\n",
    "    loc_image = loc_image.astype(np.uint8)\n",
    "    \n",
    "    return loc_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3325ff4-c4d7-4823-8812-3d1458af62f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(process_into_rgb(ds_rad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbcbd71-047e-4e0c-8df9-663a5364eb0f",
   "metadata": {},
   "source": [
    "## Reducing Dimentionality\n",
    "\n",
    "I'm just going to perform a simple PCA here to reduce the dimentionality of the image. I should be able to reduce the number of bands without distorting the spacial data and then apply the segmentation later to the original data, or an RGB representation, if I wish.\n",
    "\n",
    "The first section of code is redundant and only there for sanity checking and illustrative purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325abfb-8b21-4071-9577-fdec84cfcdb0",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81f4be-c22f-4612-a3fd-8f5d0ddb1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "if run:\n",
    "    pca = PCA()\n",
    "\n",
    "    # fit my data, and transform\n",
    "    pcX = pca.fit_transform(X)\n",
    "\n",
    "    # graph the variance of each component (should give a good idea about the number of components to use).\n",
    "    ev = pca.explained_variance_ratio_\n",
    "    cumulativeVar = np.cumsum(ev)\n",
    "    \n",
    "    plt.plot(cumulativeVar)\n",
    "    plt.xlabel('# Components')\n",
    "    plt.ylabel('Total Explained Variance')\n",
    "    plt.savefig('evaluations/sardi/'+iteration+'_pca_elbow.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491f282-cc9f-4cc5-8cbf-f284c576a2ab",
   "metadata": {},
   "source": [
    "Now to actually perform the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c35a0-f75b-40c6-a1d6-3dd6cec40b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick number of components that seems reasonable.\n",
    "nComp = 20 # between 85-90% of the variance\n",
    "pca = PCA(n_components=nComp)\n",
    "pca.fit(X)\n",
    "pcX = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e45ddb-079e-4fb6-a0a2-0658e6a8b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the new dimensions\n",
    "print(pcX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370dca74-1c3a-41de-a8bb-9c9eeaee02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat the image into the original shape\n",
    "image_pca = pcX.reshape(data.shape[0], data.shape[1], pcX.shape[1])\n",
    "print(image_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf65394-8711-4455-8aba-5cef83fb4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the reduced image, if using 3 components\n",
    "if image_pca.shape[2] == 3:\n",
    "    plt.imshow(image_pca, 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5e0a4-ce03-415e-a054-a682e2c04d57",
   "metadata": {},
   "source": [
    "## K-means elbow method to determine optimal n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dacbf4f-6518-4f10-a4c0-9136fab29201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only running this once to prove a point, it takes too long.\n",
    "run = False\n",
    "if run:\n",
    "    inertia = {}\n",
    "    for k in range(1, 10):\n",
    "        kmeans = KMeans(n_clusters=k).fit(pcX)\n",
    "        inertia[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n",
    "    plt.figure()\n",
    "    plt.plot(list(inertia.keys()), list(inertia.values()))\n",
    "    plt.xlabel(\"# Clusters\")\n",
    "    plt.ylabel(\"SSE\")\n",
    "    plt.savefig('evaluations/sardi/'+iteration+'_k-means_elbow.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96494591-1466-4573-a8ef-66f80f7cb4fa",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0da67-5ab3-4120-8eda-4cde034435f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our classifier\n",
    "k_means = KMeans(n_clusters=5) # based on elbow method above, going to stick to this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70882459-c619-416c-97cc-d627172f178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify\n",
    "k_fit = k_means.fit(pcX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece675b-d1f4-41b5-a61e-1a96b88adc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture clusers and labels\n",
    "x_labels = k_means.labels_\n",
    "x_cluster = x_labels.reshape(image_pca[:, :, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c00a8-41de-4cf5-9464-2bb0732e8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_cluster)\n",
    "plt.imsave('evaluations/sardi/'+iteration+'_classification.png', x_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ced6c8-a2a3-4cb5-a7b1-836272f12ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def change_value(img, v_from, v_to):\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            if img[i,j] == v_from:\n",
    "                img[i,j] = v_to\n",
    "    return img\n",
    "\n",
    "classified_waste = x_cluster.copy()\n",
    "                \n",
    "classified_waste = change_value(classified_waste, 0, 0)\n",
    "classified_waste = change_value(classified_waste, 1, 1)\n",
    "classified_waste = change_value(classified_waste, 2, 0)\n",
    "classified_waste = change_value(classified_waste, 3, 0)\n",
    "classified_waste = change_value(classified_waste, 4, 0)\n",
    "# classified_waste = change_value(classified_waste, 5, 0)\n",
    "            \n",
    "plt.imshow(classified_waste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f4f76-6140-4428-986f-b56a7a53b3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imsave('evaluations/sardi/'+iteration+'_class_waste.png', classified_waste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733930c-b38d-492b-8344-215c0f0d6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [np.full(x_cluster.shape,0),classified_waste,classified_waste]\n",
    "final = np.array(final).transpose(1,2,0)*255\n",
    "final = final.astype(np.uint8)\n",
    "plt.imshow(final)\n",
    "plt.imsave('evaluations/sardi/'+iteration+'_class_final.png', final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43427858-c34c-40a9-9719-f041e0f5da76",
   "metadata": {},
   "source": [
    "## Blend original and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63541ee9-9961-4c71-ab0d-bb72dad15187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original = process_into_rgb(ds_rad)\n",
    "original = original[:430,:930, :]\n",
    "plt.imshow(original)\n",
    "plt.imsave('evaluations/sardi/'+iteration+'_original.png', original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214cb89-2610-4015-881c-bd5066a26f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(original.shape)\n",
    "print(original.dtype)\n",
    "print(final.shape)\n",
    "print(final.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd33960-b685-4ef2-b584-684bdedb989c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blended = cv2.addWeighted(final, 0.6, original, 0.6, 0)\n",
    "plt.imshow(blended)\n",
    "plt.imsave('evaluations/sardi/'+iteration+'_blended.png', blended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839597e-3d18-4311-9358-07bf22632f5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compare to ground truth and RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dfecab-070d-47b3-959d-9307d4e1e3b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9ed3c-bb65-47d8-963e-6d44a2559e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv2.imread('right-pipe-rgb.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd15f63-bad3-4ee0-845a-2c675005c313",
   "metadata": {},
   "source": [
    "## Matched Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee5072-4992-4499-8930-ceb8ce37cd15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let the user choose a pixel to mach\n",
    "\n",
    "img_copy = blended.copy()\n",
    "pixel = (0,0)\n",
    "\n",
    "def on_click(event,x,y,flags,params):\n",
    "    global pixel\n",
    "    global img_copy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        img_copy = blended.copy()\n",
    "        pixel = data[y,x]\n",
    "        cv2.circle(img_copy,(x,y),5,(127,255,127),1)\n",
    "        cv2.circle(img_copy,(x,y),1,(255,127,127),1)\n",
    "    \n",
    "cv2.namedWindow('Image')\n",
    "cv2.setMouseCallback('Image', on_click)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow('Image',img_copy)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "\n",
    "    # jump out of loop and close all windows with ESC\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260cc3f-27c4-4c95-9ac9-91fa80e629a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let the user select an area to average and match\n",
    "\n",
    "# Select ROI\n",
    "r = cv2.selectROI(\"select roi\", blended)\n",
    "\n",
    "# close the window\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Crop hsi image\n",
    "cropped = data[int(r[1]):int(r[1]+r[3]),int(r[0]):int(r[0]+r[2]),:]\n",
    "\n",
    "sample_img = blended.copy()\n",
    "#                         src, startpoint, endpoint, colour, thickness\n",
    "sample_img = cv2.rectangle(sample_img, (int(r[0]),int(r[1])), (int(r[0]+r[2]),int(r[1]+r[3])), (27,27,27), 2)\n",
    "\n",
    "reshaped = cropped.reshape(cropped.shape[0]*cropped.shape[1], cropped.shape[2])\n",
    "\n",
    "# take mean of sample area\n",
    "pixel = np.mean(reshaped,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a7807-1d86-480c-9394-89099391f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the image and match on the chosen pixel\n",
    "matched = matched_filter(data, pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d2b52-e6f0-409e-8c8e-03ac05360041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the matches from the single pixel\n",
    "matched_img = 1 * (matched > 0.5)\n",
    "v = imshow(matched_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463b351-be12-4c20-9641-a020837b9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave('evaluations/sardi/'+iteration+'_mf.png', matched_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660400c3-f1b2-4afa-b282-c461fcf100ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compare to ground truth and RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecbdb6f-402c-4848-acb7-82cca0fa2410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56640b16-0592-4575-9015-df1635f16d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv2.imread('right-pipe-rgb.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fabad-d3ce-4d6a-8629-35fcc9b0c9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
